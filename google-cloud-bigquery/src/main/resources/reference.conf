alpakka.google.bigquery {

  credentials {
    # Options: service-account, compute-engine
    provider = service-account

    service-account {
      # If `path` is non-empty then read from file, otherwise fallback to config values
      path = ""
      path = ${?GOOGLE_APPLICATION_CREDENTIALS}
      project-id = ""
      client-email = ""
      private-key = ""
      scopes = ["https://www.googleapis.com/auth/bigquery"]
    }

    # Timeout for blocking call during settings initialization to compute engine metadata server
    compute-engine.timeout = 1s
  }

  load-job {
    # The minimum size of a chunk
    chunk-size = 15 MB

    # BigQuery has a hard limit of 1,500 load jobs per table per day (just over 1 job per minute)
    # This sets the rate limit when loading data via BigQuery.insertAllAsync
    per-table-quota = 1 minute
  }

  # The retry settings for requests to Google APIs
  retry-settings {
    max-retries = 6
    min-backoff = 1 second
    max-backoff = 1 minute
    random-factor = 0.2
  }

  # An address of a proxy that will be used for all connections using HTTP CONNECT tunnel.
  # forward-proxy {
  #   scheme = "https"
  #   host = "proxy"
  #   port = 8080
  #   credentials {
  #     username = "username"
  #     password = "password"
  #   }
  #   trust-pem = "/path/to/file.pem"
  # }
}
